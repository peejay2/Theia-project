{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import nltk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Independent albertomanzi92@gmail.com'\n",
    "}\n",
    "\n",
    "links = ['https://www.sec.gov/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm',\n",
    "         'https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm',\n",
    "         'https://www.sec.gov/Archives/edgar/data/51143/000155837023002376/ibm-20221231x10k.htm']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(links):\n",
    "    #instantiate dictionary to store data\n",
    "    company_data = {}\n",
    "    for link in links:\n",
    "            # extract  ticker symbol from URL\n",
    "        ticker = re.search(r'/([^/]+)-', link).group(1)\n",
    "        company_data[ticker] = {}\n",
    "        \n",
    "        r = requests.get(link, headers=headers)\n",
    "        raw_10k = r.text\n",
    "\n",
    "        #regex to find specific items of 10-k\n",
    "        regex = re.compile(r'(>Item(\\s|&#160;|&#xA0;|&nbsp;)(1|1A|1B|2|7A|7|8|9)(?=\\.|\\s|$))|(ITEM\\s(1|&#xA00;|1A|1B|2|7A|7|8|9)(?=\\.|\\s|$))|(>Item(\\s|&#160;|&nbsp;)1(?=\\.|\\s|$))|(ITEM\\s1(?=\\.|\\s|$))')\n",
    "\n",
    "        # Use finditer to match the regex\n",
    "        matches = regex.finditer(raw_10k)\n",
    "\n",
    "        #Write a for loop to print the matches\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "            \n",
    "        # Matches\n",
    "        matches = regex.finditer(raw_10k)\n",
    "\n",
    "        # Create  dataframe\n",
    "        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "        test_df.columns = ['item', 'start', 'end']\n",
    "        test_df['item'] = test_df.item.str.lower()\n",
    "\n",
    "        # Get rid of unnecessary characters from the dataframe\n",
    "        test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "        test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "        test_df.replace(' ','',regex=True,inplace=True)\n",
    "        test_df.replace('\\.','',regex=True,inplace=True)\n",
    "        test_df.replace('>','',regex=True,inplace=True)\n",
    "\n",
    "        #remove first half of entries as they are indexes\n",
    "        pos_dat = test_df[(len(test_df)//2):]\n",
    "\n",
    "        # Set item as the dataframe index\n",
    "        pos_dat.set_index('item', inplace=True)\n",
    "\n",
    "        # display the dataframe\n",
    "        print(pos_dat)\n",
    "\n",
    "        #divide into items\n",
    "        item_1_raw = raw_10k[pos_dat['start'].iloc[0]:pos_dat['start'].iloc[1]]\n",
    "        item_1a_raw = raw_10k[pos_dat['start'].iloc[1]:pos_dat['start'].iloc[2]]\n",
    "        item_1b_raw = raw_10k[pos_dat['start'].iloc[2]:pos_dat['start'].iloc[3]]\n",
    "        item_7_raw = raw_10k[pos_dat['start'].iloc[3]:pos_dat['start'].iloc[4]]\n",
    "        item_7a_raw = raw_10k[pos_dat['start'].iloc[4]:pos_dat['start'].iloc[5]]\n",
    "        item_8_raw = raw_10k[pos_dat['start'].iloc[5]:pos_dat['start'].iloc[6]]\n",
    "        \n",
    "        item_1_content = BeautifulSoup(item_1_raw, 'lxml').get_text()        \n",
    "        item_1a_content = BeautifulSoup(item_1a_raw, 'lxml').get_text() \n",
    "        item_1b_content = BeautifulSoup(item_1b_raw, 'lxml').get_text()\n",
    "        item_7_content = BeautifulSoup(item_7_raw, 'lxml')\n",
    "        item_7a_content = BeautifulSoup(item_7a_raw, 'lxml')\n",
    "        item_8_content = BeautifulSoup(item_8_raw, 'lxml')\n",
    "        \n",
    "        #add to dictionary\n",
    "        tickerdict = {\n",
    "        'Item 1': item_1_content,\n",
    "        'Item 1A': item_1a_content,\n",
    "        'Item 1B': item_1b_content,\n",
    "        'Item 7': item_7_content,\n",
    "        'Item 7A': item_7a_content,\n",
    "        'Item 8': item_8_content\n",
    "    }\n",
    "        # add the ticker dictionary to the company data dictionary\n",
    "        company_data[ticker] = tickerdict\n",
    "        \n",
    "    return company_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data = clean_data(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_summary(input):\n",
    "    # use pegasus model pre-trained on financial data\n",
    "    from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "    # Load  model and  tokeniser \n",
    "    model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    text_to_summarize = input\n",
    "    input_ids = tokenizer(text_to_summarize, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=75, \n",
    "        num_beams=5, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Print the generated summary\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Business Summary'] = business_summary(company_data[company]['Item 1'][0:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core industry/themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import themes_df - pandas df containing industries and themes\n",
    "from themes import themes_df\n",
    "import torch\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"sileod/deberta-v3-base-tasksource-nli\", device=device, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifies text into categories\n",
    "\n",
    "def run_classifier(input, labels, classifier=classifier, threshold=0.5):\n",
    "  results = classifier(input,labels)\n",
    "  reach_threshold = len([item for item in results['scores'] if item > threshold])\n",
    "  return results['labels'][0:(max(1,reach_threshold))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively goes through themes to find sub-themes\n",
    "\n",
    "def find_themes(input_text, df):\n",
    "  themes = []\n",
    "  for sector in run_classifier(input_text, df['Sector'].unique()):\n",
    "    print(sector)\n",
    "    for industry in run_classifier(input_text, df[df['Sector'] == sector]['Industry'].unique()):\n",
    "      print(industry)\n",
    "      for subindustry in run_classifier(input_text, df[(df['Sector'] == sector) & (df['Industry'] == industry)]['Sub-Industry'].unique()):\n",
    "        print(subindustry)\n",
    "        for theme in run_classifier(input_text, df[(df['Sector'] == sector) & (df['Industry'] == industry) & (df['Sub-Industry'] == subindustry)]['Theme'].unique()):\n",
    "          themes.append(df[df['Theme'] == theme].values.tolist()[0])\n",
    "  return themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "     company_data[company]['Themes'] = find_themes(company_data[company]['Item 1'],themes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core products/services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\Documents\\Theia take home project\\theia_project\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#import gliner one-shot named entity recognition model\n",
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "\n",
    "labels = [\"products\", \"services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prods(company, model=model, labels=labels):\n",
    "    input = company['Item 1']\n",
    "    products = []\n",
    "    services = []\n",
    "    entities = model.predict_entities(input, labels)\n",
    "    for entity in entities:\n",
    "        print(entity[\"text\"], \"=>\", entity[\"label\"])\n",
    "        if entity['label'] == 'products':\n",
    "            products.append(entity['text'])\n",
    "        elif entity['label'] == 'services':\n",
    "            services.append(entity['text'])\n",
    "    print(products)\n",
    "    print(services)\n",
    "    return products,services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Products'],company_data[company]['Services'] = find_prods(company_data[company])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data['tsla']['Services']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers/market segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(company):\n",
    "    text = company['Item 1']\n",
    "    topics = [\n",
    "    \"Company overview\", \"Segment information\", \"Products and services\", \"Technology\", \"Sales and marketing\", \"Service and warranty\", \"Manufacturing\",\n",
    "    \"Supply chain\", \"Government programs, incentives, and regulations\", \"Competition\", \"Customers\", \"Demographics\", \"Target market\"]\n",
    "    \n",
    "    #break down input into iterable\n",
    "    iterabletext = nltk.sent_tokenize(text)  \n",
    "    print(len(iterabletext))\n",
    "    topic_model = BERTopic(\n",
    "    embedding_model=\"all-minilm-l6-v2\", \n",
    "    min_topic_size=15,\n",
    "    zeroshot_topic_list=topics,\n",
    "    zeroshot_min_similarity=.85,\n",
    "    representation_model=KeyBERTInspired()\n",
    ")\n",
    "    topics, _ = topic_model.fit_transform(iterabletext)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Market segments'] = find_topics(company_data[company])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_financials(company):\n",
    "    #extract to beautifulsoup object\n",
    "    tables = company['Item 8'].find_all('table')\n",
    "    dfs = []\n",
    "    for table in tables:\n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    #drop columns with more than half NA values\n",
    "    for index,df in enumerate(dfs):\n",
    "        threshold = df.shape[1]//2\n",
    "        df = df.dropna(thresh=threshold)\n",
    "        dfs[index] = df    \n",
    "        \n",
    "    #keep only first 8 tables \n",
    "    if len(dfs) > 8:\n",
    "        dfs = dfs[0:8]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Financials'] = return_financials(company_data[company])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    print(company_data[company].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theia_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
