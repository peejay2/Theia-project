{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import nltk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Independent albertomanzi92@gmail.com'\n",
    "}\n",
    "\n",
    "links = ['https://www.sec.gov/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm',\n",
    "         'https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm',\n",
    "         'https://www.sec.gov/Archives/edgar/data/51143/000155837023002376/ibm-20221231x10k.htm']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(links):\n",
    "    company_data = {}\n",
    "    for link in links:\n",
    "            # extract  ticker symbol from URL\n",
    "        ticker = re.search(r'/([^/]+)-', link).group(1)\n",
    "        company_data[ticker] = {}\n",
    "        \n",
    "        r = requests.get(link, headers=headers)\n",
    "        raw_10k = r.text\n",
    "        \n",
    "        # nltk.sent_tokenize(raw_10k)\n",
    "        # raw_10k = \"\\n\".join(raw_10k)\n",
    "        \n",
    "        #regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1|1A|1B|2|7A|7|8|9)(?=\\.|\\s|$))|(ITEM\\s(1|1A|1B|2|7A|7|8|9)(?=\\.|\\s|$))')\n",
    "        regex = re.compile(r'(>Item(\\s|&#160;|&#xA0;|&nbsp;)(1|1A|1B|2|7A|7|8|9)(?=\\.|\\s|$))|(ITEM\\s(1|&#xA00;|1A|1B|2|7A|7|8|9)(?=\\.|\\s|$))|(>Item(\\s|&#160;|&nbsp;)1(?=\\.|\\s|$))|(ITEM\\s1(?=\\.|\\s|$))')\n",
    "\n",
    "        # Use finditer to match the regex\n",
    "        matches = regex.finditer(raw_10k)\n",
    "\n",
    "        #Write a for loop to print the matches\n",
    "        # for match in matches:\n",
    "        #     print(match)\n",
    "            \n",
    "        # Matches\n",
    "        matches = regex.finditer(raw_10k)\n",
    "\n",
    "        # Create the dataframe\n",
    "        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "\n",
    "        test_df.columns = ['item', 'start', 'end']\n",
    "        test_df['item'] = test_df.item.str.lower()\n",
    "\n",
    "        # Get rid of unnecessary characters from the dataframe\n",
    "        test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "        test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "        test_df.replace(' ','',regex=True,inplace=True)\n",
    "        test_df.replace('\\.','',regex=True,inplace=True)\n",
    "        test_df.replace('>','',regex=True,inplace=True)\n",
    "\n",
    "        #remove first half of entries as they are indexes\n",
    "        # pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n",
    "        pos_dat = test_df[(len(test_df)//2):]\n",
    "\n",
    "        # Set item as the dataframe index\n",
    "        pos_dat.set_index('item', inplace=True)\n",
    "\n",
    "        # display the dataframe\n",
    "        print(pos_dat)\n",
    "\n",
    "        item_1_raw = raw_10k[pos_dat['start'].iloc[0]:pos_dat['start'].iloc[1]]\n",
    "        item_1a_raw = raw_10k[pos_dat['start'].iloc[1]:pos_dat['start'].iloc[2]]\n",
    "        item_1b_raw = raw_10k[pos_dat['start'].iloc[2]:pos_dat['start'].iloc[3]]\n",
    "        item_7_raw = raw_10k[pos_dat['start'].iloc[3]:pos_dat['start'].iloc[4]]\n",
    "        item_7a_raw = raw_10k[pos_dat['start'].iloc[4]:pos_dat['start'].iloc[5]]\n",
    "        item_8_raw = raw_10k[pos_dat['start'].iloc[5]:pos_dat['start'].iloc[6]]\n",
    "        \n",
    "        item_1_content = BeautifulSoup(item_1_raw, 'lxml').get_text()        \n",
    "        item_1a_content = BeautifulSoup(item_1a_raw, 'lxml').get_text() \n",
    "        item_1b_content = BeautifulSoup(item_1b_raw, 'lxml').get_text()\n",
    "        item_7_content = BeautifulSoup(item_7_raw, 'lxml')\n",
    "        item_7a_content = BeautifulSoup(item_7a_raw, 'lxml')\n",
    "        item_8_content = BeautifulSoup(item_8_raw, 'lxml')\n",
    "        \n",
    "        tickerdict = {\n",
    "        'Item 1': item_1_content,\n",
    "        'Item 1A': item_1a_content,\n",
    "        'Item 1B': item_1b_content,\n",
    "        'Item 7': item_7_content,\n",
    "        'Item 7A': item_7a_content,\n",
    "        'Item 8': item_8_content\n",
    "    }\n",
    "        # add the ticker dictionary to the company data dictionary\n",
    "        company_data[ticker] = tickerdict\n",
    "        \n",
    "    return company_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data = clean_data(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data['aapl']['Item 1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_summary(input):\n",
    "    from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n",
    "\n",
    "    # Load  model and  tokeniser \n",
    "    model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    # Some text to summarize here\n",
    "    text_to_summarize = input\n",
    "    input_ids = tokenizer(text_to_summarize, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=75, \n",
    "        num_beams=5, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Print the generated summary\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for company in company_data:\n",
    "#     company_data[company]['Business Summary'] = business_summary(company_data[company]['Item 1'][0:400])\n",
    "business_summary(company_data['ibm']['Item 1'][0:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core industry/themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from themes import themes_df\n",
    "import torch\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"sileod/deberta-v3-base-tasksource-nli\", device=device, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifies text into categories\n",
    "\n",
    "def run_classifier(input, labels, classifier=classifier, threshold=0.5):\n",
    "  results = classifier(input,labels)\n",
    "  reach_threshold = len([item for item in results['scores'] if item > threshold])\n",
    "  return results['labels'][0:(max(1,reach_threshold))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively goes through themes to find sub-themes\n",
    "\n",
    "def find_themes(input_text, df):\n",
    "  themes = []\n",
    "  for sector in run_classifier(input_text, df['Sector'].unique()):\n",
    "    print(sector)\n",
    "    for industry in run_classifier(input_text, df[df['Sector'] == sector]['Industry'].unique()):\n",
    "      print(industry)\n",
    "      for subindustry in run_classifier(input_text, df[(df['Sector'] == sector) & (df['Industry'] == industry)]['Sub-Industry'].unique()):\n",
    "        print(subindustry)\n",
    "        for theme in run_classifier(input_text, df[(df['Sector'] == sector) & (df['Industry'] == industry) & (df['Sub-Industry'] == subindustry)]['Theme'].unique()):\n",
    "          themes.append(df[df['Theme'] == theme].values.tolist()[0])\n",
    "  return themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "     company_data[company]['Themes'] = find_themes(company_data[company]['Item 1'],themes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core products/services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "\n",
    "labels = [\"products\", \"services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prods(company, model=model, labels=labels):\n",
    "    input = company['Item 1']\n",
    "    products = []\n",
    "    services = []\n",
    "    entities = model.predict_entities(input, labels)\n",
    "    for entity in entities:\n",
    "        print(entity[\"text\"], \"=>\", entity[\"label\"])\n",
    "        if entity['label'] == 'products':\n",
    "            products.append(entity['text'])\n",
    "        elif entity['label'] == 'services':\n",
    "            services.append(entity['text'])\n",
    "    print(products)\n",
    "    print(services)\n",
    "    # company_data[company]['Products'] = products\n",
    "    # company_data[company]['Services'] = services\n",
    "    return products,services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Products'],company_data[company]['Services'] = find_prods(company_data[company])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data['tsla']['Services']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers/market segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(company):\n",
    "    text = company['Item 1']\n",
    "    topics = [\n",
    "    \"Company overview\", \"Segment information\", \"Products and services\", \"Technology\", \"Sales and marketing\", \"Service and warranty\", \"Manufacturing\",\n",
    "    \"Supply chain\", \"Government programs, incentives, and regulations\", \"Competition\", \"Customers\", \"Demographics\", \"Target market\"]\n",
    "    \n",
    "    #break down input into iterable\n",
    "    iterabletext = nltk.sent_tokenize(text)  \n",
    "    print(len(iterabletext))\n",
    "    topic_model = BERTopic(\n",
    "    embedding_model=\"all-minilm-l6-v2\", \n",
    "    min_topic_size=15,\n",
    "    zeroshot_topic_list=topics,\n",
    "    zeroshot_min_similarity=.85,\n",
    "    representation_model=KeyBERTInspired()\n",
    ")\n",
    "    topics, _ = topic_model.fit_transform(iterabletext)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Market segments'] = find_topics(company_data[company])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data['ibm']['Item 1A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data['tsla'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=\"thenlper/gte-small\", \n",
    "    min_topic_size=15,\n",
    "    zeroshot_topic_list=topics,\n",
    "    zeroshot_min_similarity=.85,\n",
    "    representation_model=KeyBERTInspired()\n",
    ")\n",
    "topics, _ = topic_model.fit_transform(iterabletext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterabletext = [i for i in text.split('\\n\\n') if len(i) > 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_financials(company):\n",
    "    #extract to beautifulsoup object\n",
    "    tables = company['Item 8'].find_all('table')\n",
    "    dfs = []\n",
    "    for table in tables:\n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    #drop columns with more than half NA values\n",
    "    for index,df in enumerate(dfs):\n",
    "        threshold = df.shape[1]//2\n",
    "        df = df.dropna(thresh=threshold)\n",
    "        dfs[index] = df    \n",
    "        \n",
    "    #keep only first 8 tables \n",
    "    if len(dfs) > 8:\n",
    "        dfs = dfs[0:8]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    company_data[company]['Financials'] = return_financials(company_data[company])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_data:\n",
    "    print(company_data[company].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theia_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
